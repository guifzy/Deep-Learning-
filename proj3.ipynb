{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":4804396,"sourceType":"datasetVersion","datasetId":2779739},{"sourceId":13470560,"sourceType":"datasetVersion","datasetId":8551172},{"sourceId":13639535,"sourceType":"datasetVersion","datasetId":8669948},{"sourceId":629522,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":474246,"modelId":490129}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport torch\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, random_split, Subset\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-06T01:13:50.895182Z","iopub.execute_input":"2025-11-06T01:13:50.896068Z","iopub.status.idle":"2025-11-06T01:13:59.361489Z","shell.execute_reply.started":"2025-11-06T01:13:50.896043Z","shell.execute_reply":"2025-11-06T01:13:59.360840Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Sobre os dados","metadata":{}},{"cell_type":"markdown","source":"O WikiArt Art Movements/Styles é um conjunto de dados amplamente utilizado em pesquisas de classificação de estilos artísticos e aprendizado de representação visual.\nEle foi construído a partir do site WikiArt.org\n, que reúne obras digitalizadas de centenas de artistas de diversas épocas, estilos e movimentos artísticos.","metadata":{}},{"cell_type":"markdown","source":"Sobre os conjuntos selecionados:\n\n- Japanese Art \n\n    - Traços planos, uso de linhas limpas, pouca profundidade, tons pastéis, e forte presença de natureza e figuras estilizadas.\n\n- Baroque \n\n    - Pinturas escuras, com forte contraste de luz e sombra (chiaroscuro), cenas dramáticas, expressões intensas e composição teatral.\n\n- Art Nouveau \n\n    - Linhas curvas, formas orgânicas, motivos florais e cores suaves.\n\n- Primitivism (Naïve Art)\n\n    - Formas simplificadas, ausência de perspectiva correta, cores vibrantes e figuras \"infantis\".\n\n- Renaissance (Western)\n\n    - Equilíbrio, perspectiva linear, realismo anatômico, temas religiosos ou mitológicos.","metadata":{}},{"cell_type":"markdown","source":"A escolha dos cinco estilos citados baseia-se em critérios de separabilidade visual, otimizando a aprendizagem do modelo e reduzindo ambiguidades.\n\n\nCada categoria apresenta:\n\n- Paleta de cores distinta,\n\n- Nível de detalhamento próprio,\n\n- Estrutura composicional diferenciada,\n\n- Referenciais culturais únicos.\n\nIsso reduz o overlap entre classes, facilitando a convergência do modelo e a avaliação da acurácia real de classificação.","metadata":{}},{"cell_type":"markdown","source":"# Carregamento","metadata":{}},{"cell_type":"code","source":"from torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, random_split\n\n# Transformação base (sem augmentation)\nbase_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n])\n#dir = \"/kaggle/input/pinturas-df/database\"\n#dataset = datasets.ImageFolder(dir, transform=base_transform)\n\n#print(\"Classes:\", dataset.classes)\n#print(\"Total de imagens:\", len(dataset))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T01:13:59.363230Z","iopub.execute_input":"2025-11-06T01:13:59.363528Z","iopub.status.idle":"2025-11-06T01:13:59.368082Z","shell.execute_reply.started":"2025-11-06T01:13:59.363512Z","shell.execute_reply":"2025-11-06T01:13:59.367066Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from collections import Counter\n\n# class_counts = Counter([label for _, label in dataset])\n# for cls, count in zip(dataset.classes, class_counts.values()):\n    # print(f\"{cls}: {count} imagens\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T01:13:59.369197Z","iopub.execute_input":"2025-11-06T01:13:59.369502Z","iopub.status.idle":"2025-11-06T01:13:59.405744Z","shell.execute_reply.started":"2025-11-06T01:13:59.369476Z","shell.execute_reply":"2025-11-06T01:13:59.404956Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Art_Nouveau: 3035 imagens\n\nBaroque: 5312 imagens\n\nJapanese_Art: 2235 imagens\n\nPrimitivism: 1324 imagens\n\nRenaissance: 6192 imagens","metadata":{}},{"cell_type":"markdown","source":"- Calculando a média e o desvio padrão para normalização das imagens","metadata":{}},{"cell_type":"code","source":"\n#from tqdm import tqdm\n\n#loader = DataLoader(dataset, batch_size=64, shuffle=False, num_workers=4)\n\n#mean = torch.zeros(3)\n#std = torch.zeros(3)\n\n#print(\"Calculando média e desvio padrão...\")\n#for images, _ in tqdm(loader):\n    # images.shape = [batch, channels, height, width]\n    #batch_samples = images.size(0)\n    #images = images.view(batch_samples, images.size(1), -1)  # [batch, channels, pixels]\n    #mean += images.mean(2).sum(0)\n    #std += images.std(2).sum(0)\n\n#mean /= len(dataset)\n#std /= len(dataset)\n\n#print(f\"\\nMédia por canal: {mean}\")\n#print(f\"Desvio padrão por canal: {std}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T01:13:59.406510Z","iopub.execute_input":"2025-11-06T01:13:59.406780Z","iopub.status.idle":"2025-11-06T01:13:59.420239Z","shell.execute_reply.started":"2025-11-06T01:13:59.406763Z","shell.execute_reply":"2025-11-06T01:13:59.419664Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> Média e desvio padrão dos canais:\n\n> mean=[0.4997, 0.4385, 0.3752]\n> \n> std=[0.2115, 0.1954, 0.1748]","metadata":{}},{"cell_type":"markdown","source":"## Realizando processamento e augmentation","metadata":{}},{"cell_type":"code","source":"dir = \"/kaggle/input/pinturas-df/database\"\nbase_dataset = datasets.ImageFolder(dir)\n\nclasses_desejadas = ['Art_Nouveau', 'Baroque', 'Japanese_Art', 'Primitivism']\n\n# Filtra os índices das classes desejadas\nindices_filtrados = [\n    i for i, (_, label) in enumerate(base_dataset)\n    if base_dataset.classes[label] in classes_desejadas\n]\n\n# Cria o subset filtrado\ndataset = Subset(base_dataset, indices_filtrados)\n\n# Atualiza as classes e o mapeamento\ndataset.dataset.classes = classes_desejadas\ndataset.dataset.class_to_idx = {cls: i for i, cls in enumerate(classes_desejadas)}\n\nprint(\"Classes escolhidas:\", dataset.dataset.classes)\nprint(\"Total de imagens após filtro:\", len(dataset))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T01:13:59.421010Z","iopub.execute_input":"2025-11-06T01:13:59.421252Z","iopub.status.idle":"2025-11-06T01:25:12.706173Z","shell.execute_reply.started":"2025-11-06T01:13:59.421226Z","shell.execute_reply":"2025-11-06T01:25:12.705343Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"augmentations = {\n    \"Art_Nouveau\": transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.RandomResizedCrop(224, scale=(0.9, 1.0)),\n        transforms.RandomHorizontalFlip(p=0.4),\n        transforms.RandomRotation(15),\n        transforms.ColorJitter(brightness=0.1, contrast=0.1),\n        transforms.RandomPerspective(distortion_scale=0.05, p=0.5),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.4997, 0.4385, 0.3752],\n                             std=[0.2115, 0.1954, 0.1748]),\n        transforms.RandomErasing(p=0.3)\n    ]),\n    \"Japanese_Art\": transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.RandomResizedCrop(224, scale=(0.9, 1.0)),\n        transforms.RandomHorizontalFlip(p=0.4),\n        transforms.RandomRotation(15),\n        transforms.ColorJitter(brightness=0.1, contrast=0.1),\n        transforms.RandomPerspective(distortion_scale=0.05, p=0.5),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.4997, 0.4385, 0.3752],\n                             std=[0.2115, 0.1954, 0.1748]),\n        transforms.RandomErasing(p=0.3)\n        \n    ]),\n    \"Primitivism\": transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.RandomResizedCrop(224, scale=(0.9, 1.0)),\n        transforms.RandomHorizontalFlip(p=0.4),\n        transforms.RandomRotation(15),\n        transforms.ColorJitter(brightness=0.1, contrast=0.1),\n        transforms.RandomPerspective(distortion_scale=0.05, p=0.5),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.4997, 0.4385, 0.3752],\n                             std=[0.2115, 0.1954, 0.1748]),\n        transforms.RandomErasing(p=0.3)\n    ]),\n    \"Baroque\": transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.4997, 0.4385, 0.3752],\n                             std=[0.2115, 0.1954, 0.1748]),\n        transforms.RandomErasing(p=0.3)\n    ])\n}\n\nbase_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.4997, 0.4385, 0.3752],\n                         std=[0.2115, 0.1954, 0.1748]),\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T01:25:12.707103Z","iopub.execute_input":"2025-11-06T01:25:12.707717Z","iopub.status.idle":"2025-11-06T01:25:12.716224Z","shell.execute_reply.started":"2025-11-06T01:25:12.707694Z","shell.execute_reply":"2025-11-06T01:25:12.715491Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class AugmentationClasse():\n    def __init__(self, dataset, indices, augmentations, base_transform):\n        self.dataset = dataset\n        self.indices = indices\n        self.augmentations = augmentations\n        self.base_transform = base_transform\n        self.class_to_idx = dataset.dataset.class_to_idx\n        self.idx_to_class = {v: k for k, v in self.class_to_idx.items()}\n\n    def __len__(self):\n        return len(self.indices)\n\n    def __getitem__(self, idx):\n        real_idx = self.indices[idx]\n        img, label = self.dataset[real_idx]\n        class_name = self.idx_to_class[label]\n\n        transform = self.augmentations.get(class_name, self.base_transform)\n        img = transform(img)\n        return img, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T01:25:12.718338Z","iopub.execute_input":"2025-11-06T01:25:12.718512Z","iopub.status.idle":"2025-11-06T01:25:12.736444Z","shell.execute_reply.started":"2025-11-06T01:25:12.718499Z","shell.execute_reply":"2025-11-06T01:25:12.735908Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"labels = [base_dataset.imgs[i][1] for i in indices_filtrados]\n\n# Split\ntrain_idx, test_idx = train_test_split(\n    list(range(len(dataset))),\n    test_size=0.2,\n    stratify=labels,\n    random_state=42\n)\n\ntrain_dataset = AugmentationClasse(dataset, train_idx, augmentations, base_transform)\ntest_dataset = AugmentationClasse(dataset, test_idx, {}, base_transform)\n\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T01:25:12.737016Z","iopub.execute_input":"2025-11-06T01:25:12.737265Z","iopub.status.idle":"2025-11-06T01:25:12.770347Z","shell.execute_reply.started":"2025-11-06T01:25:12.737247Z","shell.execute_reply":"2025-11-06T01:25:12.769466Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from collections import Counter\nimport matplotlib.pyplot as plt\n\ndef plot_class_distribution(dataset, title):\n    class_counts = Counter()\n    for _, label in dataset:\n        class_counts[label] += 1\n    idx_to_class = dataset.idx_to_class\n    class_names = [idx_to_class[i] for i in sorted(idx_to_class.keys())]\n    counts = [class_counts[i] for i in range(len(class_names))]\n\n    plt.figure(figsize=(8, 4))\n    plt.bar(class_names, counts, color=\"skyblue\")\n    plt.title(title)\n    plt.ylabel(\"Quantidade de imagens\")\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.show()\n\n\n# print(\"Distribuição no treino:\")\n# plot_class_distribution(train_dataset, \"Distribuição Estratificada (Treino)\")\n\n# print(\"Distribuição no teste:\")\n# plot_class_distribution(test_dataset, \"Distribuição Estratificada (Teste)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T01:25:12.771067Z","iopub.execute_input":"2025-11-06T01:25:12.771273Z","iopub.status.idle":"2025-11-06T01:25:12.777771Z","shell.execute_reply.started":"2025-11-06T01:25:12.771258Z","shell.execute_reply":"2025-11-06T01:25:12.777078Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Criando oversample para as classes minirotarias ","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import WeightedRandomSampler\n\ntrain_labels = [dataset.dataset.imgs[i][1] for i in train_idx]\nclass_counts = Counter(train_labels)\n\n# peso inversamente proporcional à frequência\nnum_samples = len(train_labels)\nclass_weights = {cls: num_samples / count for cls, count in class_counts.items()}\n\nsample_weights = [class_weights[label] for label in train_labels]\n\nsampler = WeightedRandomSampler(\n    weights=sample_weights,\n    num_samples=len(sample_weights),  # mantém mesmo tamanho do treino\n    replacement=True  # permite repetição (oversampling)\n)\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=32,\n    sampler=sampler,\n    num_workers=4\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T01:25:12.778596Z","iopub.execute_input":"2025-11-06T01:25:12.778887Z","iopub.status.idle":"2025-11-06T01:25:12.796647Z","shell.execute_reply.started":"2025-11-06T01:25:12.778865Z","shell.execute_reply":"2025-11-06T01:25:12.796023Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport torch\n\ndef denormalize(img_tensor, mean, std):\n    mean = torch.tensor(mean).view(3, 1, 1)\n    std = torch.tensor(std).view(3, 1, 1)\n    return img_tensor * std + mean\n\nmean = [0.4997, 0.4385, 0.3752]\nstd = [0.2115, 0.1954, 0.1748]\n\nnum_imgs = 5\n\n# plt.figure(figsize=(20, 10))\n# for i in range(num_imgs):\n    #img, label = train_dataset[i]  # obtém uma imagem augmentada e o rótulo\n    #class_name = train_dataset.idx_to_class[label]\n\n    # Desnormaliza e converte para formato (H, W, C)\n    #img_denorm = denormalize(img, mean, std).permute(1, 2, 0).clamp(0, 1)\n\n    #plt.subplot(1, num_imgs, i + 1)\n    #plt.imshow(img_denorm)\n    #plt.title(f\"{class_name}\", fontsize=12)\n    #plt.axis(\"off\")\n\n#plt.suptitle(\"Visualização das Aumentações por Classe\", fontsize=14)\n#plt.tight_layout()\n#plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T01:25:12.797254Z","iopub.execute_input":"2025-11-06T01:25:12.797405Z","iopub.status.idle":"2025-11-06T01:25:12.811416Z","shell.execute_reply.started":"2025-11-06T01:25:12.797392Z","shell.execute_reply":"2025-11-06T01:25:12.810694Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_samples_to_simulate = 2000  # número de amostras para simular\nsimulated_indices = list(sampler)[:num_samples_to_simulate]\n\n# Recupera as classes correspondentes\nsimulated_labels = [train_labels[i] for i in simulated_indices]\n\n# Conta quantas vezes cada classe foi sorteada\nsimulated_counts = Counter(simulated_labels)\n\n# Plota a distribuição simulada\nplt.figure(figsize=(10, 5))\nplt.bar(\n    [dataset.dataset.classes[i] for i in simulated_counts.keys()],\n    simulated_counts.values(),\n    color=\"skyblue\"\n)\nplt.title(\"Distribuição simulada após WeightedRandomSampler (oversampling aplicado)\")\nplt.xlabel(\"Classes\")\nplt.ylabel(\"Número de amostras simuladas\")\nplt.xticks(rotation=45)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T01:25:12.812025Z","iopub.execute_input":"2025-11-06T01:25:12.812306Z","iopub.status.idle":"2025-11-06T01:25:13.033146Z","shell.execute_reply.started":"2025-11-06T01:25:12.812290Z","shell.execute_reply":"2025-11-06T01:25:13.032442Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Recuperando o vetor latente através das características extraídas pela CNN","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass PinturasCNN(nn.Module):\n    def __init__(self, num_classes, latent_dim=128, dropout_p=0.6):\n        super().__init__()\n\n        # inicializacao\n        self.stem = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True)\n        )\n\n        # convulcoes\n        self.block1 = self._conv_block(32, 64)\n        self.block2 = self._conv_block(64, 128)\n        self.block3 = self._conv_block(128, 192)\n\n        # pooling global 1x1 pra gerar um vetor latente mais leve\n        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n\n        # camadas pra previsao\n        self.fc_latent = nn.Linear(192, latent_dim)\n        self.bn_latent = nn.BatchNorm1d(latent_dim)\n        self.dropout = nn.Dropout(dropout_p)\n        self.fc_out = nn.Linear(latent_dim, num_classes)\n\n        # iniciando kaming\n        self._init_weights()\n\n    def _conv_block(self, in_ch, out_ch):\n        return nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=False),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=False),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n\n    def _init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n            elif isinstance(m, nn.Linear):\n                nn.init.xavier_uniform_(m.weight)\n\n    def forward(self, x, return_latent=False):\n        x = self.stem(x)\n        x = self.block1(x)\n        x = self.block2(x)\n        x = self.block3(x)\n        x = self.gap(x)\n        x = torch.flatten(x, 1)\n\n        latent = self.fc_latent(x)\n        latent = self.bn_latent(latent)\n        latent = F.relu(latent)\n\n        x = self.dropout(latent)\n        logits = self.fc_out(x)\n        return logits, latent\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T01:25:13.033934Z","iopub.execute_input":"2025-11-06T01:25:13.034181Z","iopub.status.idle":"2025-11-06T01:25:13.044320Z","shell.execute_reply.started":"2025-11-06T01:25:13.034142Z","shell.execute_reply":"2025-11-06T01:25:13.043614Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Treinamento","metadata":{}},{"cell_type":"code","source":"import torch.optim as optim\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nnum_classes = len(dataset.dataset.classes)\nlatent_dim = 256\nlearning_rate = 3e-4\nepochs = 100\nearly_stop_patience = 10\ncheckpoint_path = \"/kaggle/working/melhor_modelo.pth\"\n\nmodel = PinturasCNN(num_classes=num_classes, latent_dim=latent_dim).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T01:25:13.044970Z","iopub.execute_input":"2025-11-06T01:25:13.045184Z","iopub.status.idle":"2025-11-06T01:25:13.363710Z","shell.execute_reply.started":"2025-11-06T01:25:13.045142Z","shell.execute_reply":"2025-11-06T01:25:13.362928Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\nfrom torch.cuda.amp import autocast, GradScaler\nimport torch\nimport numpy as np\nfrom tqdm import tqdm\n\ncriterion = nn.CrossEntropyLoss(label_smoothing=0.1) \noptimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=5e-4)\nscheduler = optim.lr_scheduler.CosineAnnealingLR(\n    optimizer,\n    T_max=15,         # número de épocas até um ciclo completo\n    eta_min=1e-5      # LR mínimo\n)\nscaler = torch.amp.GradScaler('cuda' if torch.cuda.is_available() else 'cpu')  # combina float16 e float32 pra acelerar o treinamento\ndevice_type = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nbest_val_loss = np.inf\npatience_counter = 0\n\nfor epoch in range(1, epochs + 1):\n    model.train()\n    train_loss, correct, total = 0.0, 0, 0\n\n    for imgs, labels in tqdm(train_loader, desc=f\"Treino Época {epoch}/{epochs}\"):\n        imgs, labels = imgs.to(device), labels.to(device)\n        optimizer.zero_grad()\n\n        with torch.amp.autocast(device_type=device_type):\n            logits, _ = model(imgs)\n            loss = criterion(logits, labels)\n\n        scaler.scale(loss).backward()\n        scaler.unscale_(optimizer)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)  # evita exploding grads\n        scaler.step(optimizer)\n        scaler.update()\n\n        train_loss += loss.item() * imgs.size(0)\n        preds = logits.argmax(1)\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n\n    train_loss /= total\n    train_acc = correct / total\n\n    # validacao\n    model.eval()\n    val_loss, val_correct, val_total = 0.0, 0, 0\n    with torch.no_grad(), torch.amp.autocast(device_type=device_type):\n        for imgs, labels in tqdm(test_loader, desc=\"Validação\"):\n            imgs, labels = imgs.to(device), labels.to(device)\n            logits, _ = model(imgs)\n            loss = criterion(logits, labels)\n\n            val_loss += loss.item() * imgs.size(0)\n            preds = logits.argmax(1)\n            val_correct += (preds == labels).sum().item()\n            val_total += labels.size(0)\n\n    val_loss /= val_total\n    val_acc = val_correct / val_total\n    scheduler.step(val_loss)\n\n    print(f\"Época {epoch}: Loss Treino={train_loss:.4f}, Acc Treino={train_acc:.3f}, Loss Val={val_loss:.4f}, Acc Val={val_acc:.3f}\")\n\n    if val_loss < best_val_loss - 1e-4:\n        best_val_loss = val_loss\n        patience_counter = 0\n        torch.save(model.state_dict(), checkpoint_path)\n        print(\"Melhor modelo salvo!\")\n    else:\n        patience_counter += 1\n        if patience_counter >= early_stop_patience:\n            print(\"Early stopping acionado.\")\n            break\n\nprint(\"Treino finalizado com sucesso.\")\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T01:25:13.364521Z","iopub.execute_input":"2025-11-06T01:25:13.364760Z","iopub.status.idle":"2025-11-06T01:25:13.370478Z","shell.execute_reply.started":"2025-11-06T01:25:13.364736Z","shell.execute_reply":"2025-11-06T01:25:13.369894Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> Melhores resultados obtidos:\n\n> Época 25:\n> - Loss Treino=0.6583, Acc Treino=0.846\n> - Loss Val=0.9219, Acc Val=0.690 ","metadata":{}},{"cell_type":"markdown","source":"# Transfer Learning com ImageNet","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import models\n\nclass PinturasCNN_TL(nn.Module):\n    def __init__(self, num_classes, latent_dim=256, dropout_p=0.5, freeze_backbone=True):\n        super().__init__()\n\n        base_model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n\n        self.backbone = nn.Sequential(*list(base_model.children())[:-1])  # até o avgpool\n        backbone_out_dim = base_model.fc.in_features  # normalmente 512\n\n        if freeze_backbone:\n            for param in self.backbone.parameters():\n                param.requires_grad = False\n\n        self.fc_latent = nn.Linear(backbone_out_dim, latent_dim)\n        self.bn_latent = nn.BatchNorm1d(latent_dim)\n        self.dropout = nn.Dropout(dropout_p)\n\n        self.fc_out = nn.Linear(latent_dim, num_classes)\n\n        nn.init.kaiming_normal_(self.fc_latent.weight, nonlinearity='relu')\n        nn.init.xavier_uniform_(self.fc_out.weight)\n\n    def forward(self, x, return_latent=False):\n        # Extrator de características\n        x = self.backbone(x)               # Saída: [B, 512, 1, 1]\n        x = torch.flatten(x, 1)            # [B, 512]\n\n        # Vetor latente\n        latent = self.fc_latent(x)\n        latent = self.bn_latent(latent)\n        latent = F.relu(latent)\n\n        # Classificação\n        out = self.dropout(latent)\n        logits = self.fc_out(out)\n\n        if return_latent:\n            return logits, latent\n        return logits, latent\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T01:25:13.371232Z","iopub.execute_input":"2025-11-06T01:25:13.371464Z","iopub.status.idle":"2025-11-06T01:25:13.390502Z","shell.execute_reply.started":"2025-11-06T01:25:13.371443Z","shell.execute_reply":"2025-11-06T01:25:13.389824Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Treinemento","metadata":{}},{"cell_type":"code","source":"import torch.optim as optim\nfrom torch.cuda.amp import autocast, GradScaler\nimport numpy as np\nfrom tqdm import tqdm\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nnum_classes = len(dataset.dataset.classes)\nlatent_dim = 256\nlearning_rate = 3e-4\nepochs = 150\nearly_stop_patience = 10\ncheckpoint_path = \"/kaggle/working/melhor_modelo_tl.pth\"\n\n# Instancia o modelo\nmodel = PinturasCNN_TL(num_classes=num_classes, latent_dim=latent_dim, freeze_backbone=True).to(device)\n\ncriterion = nn.CrossEntropyLoss(label_smoothing=0.1)\noptimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=5e-4)\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-5)\nprint(device)\nscaler = torch.amp.GradScaler(device)\n\nbest_val_loss = np.inf\npatience_counter = 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T01:25:13.391092Z","iopub.execute_input":"2025-11-06T01:25:13.391332Z","iopub.status.idle":"2025-11-06T01:25:14.036680Z","shell.execute_reply.started":"2025-11-06T01:25:13.391316Z","shell.execute_reply":"2025-11-06T01:25:14.036066Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\nfor epoch in range(1, epochs + 1):\n    model.train()\n    train_loss, correct, total = 0.0, 0, 0\n\n    for imgs, labels in tqdm(train_loader, desc=f\"Treino Época {epoch}/{epochs}\"):\n        imgs, labels = imgs.to(device), labels.to(device)\n        optimizer.zero_grad()\n\n        with torch.amp.autocast('cuda'):\n            logits, _ = model(imgs)\n            loss = criterion(logits, labels)\n\n        scaler.scale(loss).backward()\n        scaler.unscale_(optimizer)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)\n        scaler.step(optimizer)\n        scaler.update()\n\n        train_loss += loss.item() * imgs.size(0)\n        preds = logits.argmax(1)\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n\n    train_loss /= total\n    train_acc = correct / total\n\n    # validacao\n    model.eval()\n    val_loss, val_correct, val_total = 0.0, 0, 0\n    with torch.no_grad(), torch.amp.autocast('cuda'):\n        for imgs, labels in tqdm(test_loader, desc=\"Validação\"):\n            imgs, labels = imgs.to(device), labels.to(device)\n            logits, _ = model(imgs)\n            loss = criterion(logits, labels)\n\n            val_loss += loss.item() * imgs.size(0)\n            preds = logits.argmax(1)\n            val_correct += (preds == labels).sum().item()\n            val_total += labels.size(0)\n\n    val_loss /= val_total\n    val_acc = val_correct / val_total\n    scheduler.step()\n\n    print(f\"Época {epoch:03d}: \"\n          f\"Loss Treino={train_loss:.4f}, Acc Treino={train_acc:.3f}, \"\n          f\"Loss Val={val_loss:.4f}, Acc Val={val_acc:.3f}\")\n\n    if val_loss < best_val_loss - 1e-4:\n        best_val_loss = val_loss\n        patience_counter = 0\n        torch.save(model.state_dict(), checkpoint_path)\n        print(\"Melhor modelo salvo!\")\n    else:\n        patience_counter += 1\n        if patience_counter >= early_stop_patience:\n            print(\"Early stopping acionado.\")\n            break\n\nprint(\"Treinamento finalizado com sucesso!\")\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T01:25:14.037428Z","iopub.execute_input":"2025-11-06T01:25:14.037891Z","iopub.status.idle":"2025-11-06T01:25:14.043240Z","shell.execute_reply.started":"2025-11-06T01:25:14.037872Z","shell.execute_reply":"2025-11-06T01:25:14.042593Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Melhor modelo:\n\n- Loss Treino=0.6288, Acc Treino=0.871 \n\n- Loss Val=0.7206, Acc Val=0.808","metadata":{}},{"cell_type":"markdown","source":"# Carregando o modelo para extração do vetor latente ","metadata":{}},{"cell_type":"code","source":"# Carregar o melhor modelo\ncaminho = '/kaggle/input/modelo-tl/pytorch/default/1/melhor_modelo_tl.pth'\nmodel.load_state_dict(torch.load(caminho))\nmodel.eval()\n\nlatents_list = []\nlabels_list = []\n\nwith torch.no_grad():\n    for imgs, labels in tqdm(test_loader, desc=\"Extraindo vetores latentes\"):\n        imgs = imgs.to(device)\n        _, latents = model(imgs, return_latent=True)\n        latents_list.append(latents.cpu())\n        labels_list.append(labels)\n\nlatents_all = torch.cat(latents_list)\nlabels_all = torch.cat(labels_list)\n\nprint(\"Latents extraídos:\", latents_all.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T01:25:14.043866Z","iopub.execute_input":"2025-11-06T01:25:14.044060Z","iopub.status.idle":"2025-11-06T01:25:47.038417Z","shell.execute_reply.started":"2025-11-06T01:25:14.044045Z","shell.execute_reply":"2025-11-06T01:25:47.037619Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.cluster import MiniBatchKMeans\nfrom sklearn.metrics import silhouette_score, normalized_mutual_info_score, adjusted_rand_score\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler, normalize\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\nX = latents_all.numpy()\ny = labels_all.numpy()\nX = normalize(X)\n\n# readuz a dimensionalidade para estabilidade do KMeans\npca = PCA(n_components=100, random_state=42)\nX_pca = pca.fit_transform(X)\n\n# igualando a quantidade de clusters por numero de classes\nn_clusters = num_classes  \n\n# MiniBatch KMeans\nkmeans = MiniBatchKMeans(\n    n_clusters=n_clusters,\n    batch_size=256,\n    random_state=42,\n    n_init='auto'\n)\nkmeans.fit(X_pca)\n\nclusters = kmeans.predict(X_pca)\n\n# metrica de agrupamento\nsilhouette_avg = silhouette_score(X_pca, clusters)\nprint(f\"Coeficiente de Silhouette Global: {silhouette_avg:.3f}\")\n\n# agrupa por classe original para ver quais são mais bem representadas\ndf_eval = pd.DataFrame({\"classe\": y, \"cluster\": clusters})\ncompactness_by_class = df_eval.groupby(\"classe\")[\"cluster\"].nunique()\nprint(\"\\nNúmero de clusters distintos por classe:\\n\", compactness_by_class)\n\n# reducao para visualização\npca_2d = PCA(n_components=2)\nX_2d = pca_2d.fit_transform(X_pca)\n\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\n\nsns.scatterplot(\n    ax=axes[0],\n    x=X_2d[:, 0],\n    y=X_2d[:, 1],\n    hue=clusters,\n    palette='tab10',\n    s=30,\n    alpha=0.8,\n    linewidth=0\n)\naxes[0].set_title(\"Clusters aprendidos pelo KMeans (PCA 2D)\", fontsize=13)\naxes[0].set_xlabel(\"Componente Principal 1\")\naxes[0].set_ylabel(\"Componente Principal 2\")\naxes[0].legend(title=\"Cluster\", bbox_to_anchor=(1.05, 1), loc='upper left')\n\nsns.scatterplot(\n    ax=axes[1],\n    x=X_2d[:, 0],\n    y=X_2d[:, 1],\n    hue=[dataset.dataset.classes[i] for i in y],\n    palette='Set2',\n    s=30,\n    alpha=0.8,\n    linewidth=0\n)\naxes[1].set_title(\"Classes originais (PCA 2D)\", fontsize=13)\naxes[1].set_xlabel(\"Componente Principal 1\")\naxes[1].set_ylabel(\"Componente Principal 2\")\naxes[1].legend(title=\"Classe\", bbox_to_anchor=(1.05, 1), loc='upper left')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T01:25:47.039390Z","iopub.execute_input":"2025-11-06T01:25:47.039617Z","iopub.status.idle":"2025-11-06T01:25:49.132450Z","shell.execute_reply.started":"2025-11-06T01:25:47.039596Z","shell.execute_reply":"2025-11-06T01:25:49.131722Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Recortando os clusters para identificar padrões ","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Subset, DataLoader\nfrom torchvision import transforms, datasets\n\nclasse_alvo = \"Baroque\"\n\ntransform_infer = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.4997, 0.4385, 0.3752],\n                         std=[0.2115, 0.1954, 0.1748]),\n])\n\ndataset_infer = datasets.ImageFolder(\n    root=dataset.dataset.root, \n    transform=transform_infer\n)\n\nidx_classe = dataset_infer.class_to_idx[classe_alvo]\nindices_classe = [\n    i for i, (_, label) in enumerate(dataset_infer.samples)\n    if label == idx_classe\n]\n\nsubset_classe = Subset(dataset_infer, indices_classe)\n\nsubset_loader = DataLoader(\n    subset_classe,\n    batch_size=32,\n    shuffle=False,\n    num_workers=4\n)\n\nprint(f\"{classe_alvo}: {len(subset_classe)} imagens selecionadas para extração de embeddings.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T01:25:49.133251Z","iopub.execute_input":"2025-11-06T01:25:49.133594Z","iopub.status.idle":"2025-11-06T01:25:57.709930Z","shell.execute_reply.started":"2025-11-06T01:25:49.133576Z","shell.execute_reply":"2025-11-06T01:25:57.709191Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom tqdm import tqdm\n\n# modelo treinado\nmodel.load_state_dict(torch.load(\"/kaggle/input/modelo-tl/pytorch/default/1/melhor_modelo_tl.pth\", map_location=device))\nmodel.eval()\n\n# embeddings latentes\nlatents = []\nwith torch.no_grad():\n    for imgs, _ in tqdm(subset_loader, desc=f\"Extraindo latentes de {classe_alvo}\"):\n        imgs = imgs.to(device)\n        _, latent = model(imgs, return_latent=True)\n        latents.append(latent.cpu().numpy())\n\nlatents = np.concatenate(latents, axis=0)\nprint(\"Latentes extraidos:\", latents.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T01:25:57.712903Z","iopub.execute_input":"2025-11-06T01:25:57.713201Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler, normalize\nfrom scipy.spatial.distance import cdist\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nscaler = StandardScaler()\nlatents_norm = scaler.fit_transform(latents)\nlatents_norm = normalize(latents_norm)\n\nk = 4  # ajuste conforme necessário\nkmeans = KMeans(n_clusters=k, random_state=42)\nclusters = kmeans.fit_predict(latents_norm)\ncentroids = kmeans.cluster_centers_\n\nprint(\"Distribuição dos subclusters:\")\nunique, counts = np.unique(clusters, return_counts=True)\nfor c, n in zip(unique, counts):\n    print(f\"Cluster {c}: {n} imagens\")\n\npca = PCA(n_components=2)\nlatents_2d = pca.fit_transform(latents_norm)\ncentroids_2d = pca.transform(centroids)\n\nplt.figure(figsize=(9, 7))\nsns.scatterplot(\n    x=latents_2d[:, 0], y=latents_2d[:, 1],\n    hue=clusters, palette=\"tab10\", s=35, alpha=0.8\n)\nplt.scatter(\n    centroids_2d[:, 0], centroids_2d[:, 1],\n    c='black', s=150, marker='X', edgecolor='white', linewidth=1.5,\n    label='Centróides'\n)\nfor i, (x, y) in enumerate(centroids_2d):\n    plt.text(x+0.02, y+0.02, f\"C{i}\", fontsize=10, fontweight=\"bold\", color=\"black\")\n\nplt.title(f\"Distribuição dos subgrupos e centróides – {classe_alvo}\")\nplt.xlabel(\"PCA 1\")\nplt.ylabel(\"PCA 2\")\nplt.legend(title=\"Cluster\", bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dist_matrix = cdist(centroids, centroids, metric='euclidean')\n\nprint(\"\\nMatriz de distâncias entre centróides:\")\nprint(np.round(dist_matrix, 3))\n\nplt.figure(figsize=(7, 5))\nsns.heatmap(dist_matrix, annot=True, fmt=\".3f\", cmap=\"Blues\",\n            xticklabels=[f\"C{i}\" for i in range(k)],\n            yticklabels=[f\"C{i}\" for i in range(k)])\nplt.title(\"Distâncias Euclidianas entre Centróides\")\nplt.xlabel(\"Cluster\")\nplt.ylabel(\"Cluster\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Selecionando os dados para DCGAN","metadata":{}},{"cell_type":"code","source":"import os\nfrom PIL import Image\nfrom torchvision.utils import save_image\n\nclusters_desejados = [0, 1]\n\nindices_clusters_desejados = [\n    i for i, c in enumerate(clusters) if c in clusters_desejados\n]\n\nprint(f\"Total de imagens selecionadas: {len(indices_clusters_desejados)}\")\n\nsubset_clusters = torch.utils.data.Subset(subset_classe, indices_clusters_desejados)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport numpy as np\n\nsave_dir = \"/kaggle/working/subset_baroque/\"\nos.makedirs(save_dir, exist_ok=True)\n\nprint(f\"Salvando imagens em: {save_dir}\")\n\nmean = torch.tensor([0.4997, 0.4385, 0.3752]).view(3, 1, 1)\nstd = torch.tensor([0.2115, 0.1954, 0.1748]).view(3, 1, 1)\n\nfor i, (img, _) in enumerate(subset_clusters):\n    img = img.clone().cpu()  \n    img = img * std + mean\n    img = torch.clamp(img, 0, 1)\n\n    save_path = os.path.join(save_dir, f\"img_{i:04d}.jpg\")\n    save_image(img, save_path)\n\nprint(f\"{len(subset_clusters)} imagens salvas em {save_dir}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Construindo gerador DCGAN","metadata":{}},{"cell_type":"code","source":"from torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\nimage_size = 128  \nbatch_size = 64\n\ntransform_gan = transforms.Compose([\n    transforms.Resize((image_size, image_size)),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomRotation(10),\n    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5]*3, [0.5]*3)  \n])\n\ndataset_gan = datasets.ImageFolder(\n    root=\"/kaggle/input/subset-dcgan\",  \n    transform=transform_gan\n)\n\ndataloader_gan = DataLoader(dataset_gan, batch_size=batch_size, shuffle=True, num_workers=4)\nprint(f\"Dataset carregado com {len(dataset_gan)} imagens.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T21:08:57.432004Z","iopub.execute_input":"2025-11-06T21:08:57.432290Z","iopub.status.idle":"2025-11-06T21:09:00.069856Z","shell.execute_reply.started":"2025-11-06T21:08:57.432268Z","shell.execute_reply":"2025-11-06T21:09:00.069067Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision.utils import make_grid, save_image\nimport os\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nvl = 128     # tam do vetor latente\nnc = 3       # rgb\nlr = 0.0002\nbeta1 = 0.5\nnum_epochs = 2000\n\nclass Generator(nn.Module):\n    def __init__(self, z_dim=128, img_channels=3):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.ConvTranspose2d(z_dim, 1024, 4, 1, 0, bias=False),  \n            nn.BatchNorm2d(1024),\n            nn.ReLU(True),\n\n            nn.ConvTranspose2d(1024, 512, 4, 2, 1, bias=False),    \n            nn.BatchNorm2d(512),\n            nn.ReLU(True),\n\n            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),     \n            nn.BatchNorm2d(256),\n            nn.ReLU(True),\n\n            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),     \n            nn.BatchNorm2d(128),\n            nn.ReLU(True),\n\n            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),      \n            nn.BatchNorm2d(64),\n            nn.ReLU(True),\n\n            nn.ConvTranspose2d(64, img_channels, 4, 2, 1, bias=False),  # -> 128x128\n            nn.Tanh()\n        )\n\n    def forward(self, z):\n        return self.net(z)\n\nclass Discriminator(nn.Module):\n    def __init__(self, img_channels=3):\n        super().__init__()\n        self.net = nn.Sequential(\n            # 3, 128, 128\n            nn.utils.spectral_norm(nn.Conv2d(img_channels, 64, 4, 2, 1, bias=False)),  # -> 64x64\n            nn.LeakyReLU(0.2, inplace=True),\n\n            nn.utils.spectral_norm(nn.Conv2d(64, 128, 4, 2, 1, bias=False)),  # -> 32x32\n            nn.LeakyReLU(0.2, inplace=True),\n\n            nn.utils.spectral_norm(nn.Conv2d(128, 256, 4, 2, 1, bias=False)),  # -> 16x16\n            nn.LeakyReLU(0.2, inplace=True),\n\n            nn.utils.spectral_norm(nn.Conv2d(256, 512, 4, 2, 1, bias=False)),  # -> 8x8\n            nn.LeakyReLU(0.2, inplace=True),\n\n            nn.utils.spectral_norm(nn.Conv2d(512, 1024, 4, 2, 1, bias=False)),  # -> 4x4\n            nn.LeakyReLU(0.2, inplace=True),\n\n            nn.utils.spectral_norm(nn.Conv2d(1024, 1, 4, 1, 0, bias=False))  # -> 1x1\n        )\n\n    def forward(self, x):\n        return self.net(x).view(-1)\n\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)\n\nnetG = Generator(z_dim=vl, img_channels=nc).to(device)\nnetD = Discriminator(img_channels=nc).to(device)\n\nnetG.apply(weights_init)\nnetD.apply(weights_init)\n\ncriterion = nn.BCEWithLogitsLoss()\n\noptimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\noptimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n\nfixed_noise = torch.randn(64, vl, 1, 1, device=device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T21:09:04.166337Z","iopub.execute_input":"2025-11-06T21:09:04.166605Z","iopub.status.idle":"2025-11-06T21:09:04.472533Z","shell.execute_reply.started":"2025-11-06T21:09:04.166584Z","shell.execute_reply":"2025-11-06T21:09:04.471997Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nsave_dir = \"/kaggle/working/img_geradas/\"\nos.makedirs(save_dir, exist_ok=True)\n\nfor epoch in range(num_epochs):\n    for i, (imgs, _) in enumerate(dataloader_gan):\n        real_imgs = imgs.to(device)\n        b_size = real_imgs.size(0)\n\n        # label smoothing para estabilidade\n        real_labels = torch.full((b_size,), 0.9, device=device)\n        fake_labels = torch.full((b_size,), 0.0, device=device)\n\n        optimizerD.zero_grad()\n\n        output_real = netD(real_imgs)\n        loss_real = criterion(output_real, real_labels)\n\n        noise = torch.randn(b_size, vl, 1, 1, device=device)\n        fake_imgs = netG(noise)\n        output_fake = netD(fake_imgs.detach())\n        loss_fake = criterion(output_fake, fake_labels)\n\n        loss_D = loss_real + loss_fake\n        loss_D.backward()\n        optimizerD.step()\n\n        optimizerG.zero_grad()\n        output = netD(fake_imgs)\n        loss_G = criterion(output, real_labels)\n        loss_G.backward()\n        optimizerG.step()\n\n    with torch.no_grad():\n        fake = netG(fixed_noise).detach().cpu()\n    grid = make_grid(fake, padding=2, normalize=True)\n    save_image(grid, os.path.join(save_dir, f\"epoch_{epoch+1:03d}.png\"))\n    \n    plt.figure(figsize=(6,6))\n    plt.axis(\"off\")\n    plt.title(f\"Imagens Geradas - Época {epoch+1}\")\n    plt.imshow(grid.permute(1, 2, 0).numpy())\n    plt.show()\n    \n    print(f\"[{epoch+1}/{num_epochs}] Loss D: {loss_D:.4f}, Loss G: {loss_G:.4f}\")\n\nprint(\"Treinamento concluído e imagens salvas em:\", save_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T21:09:07.492641Z","iopub.execute_input":"2025-11-06T21:09:07.492919Z","iopub.status.idle":"2025-11-06T21:09:44.703884Z","shell.execute_reply.started":"2025-11-06T21:09:07.492897Z","shell.execute_reply":"2025-11-06T21:09:44.702854Z"}},"outputs":[],"execution_count":null}]}